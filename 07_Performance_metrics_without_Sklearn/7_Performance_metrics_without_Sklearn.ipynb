{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s0Ej_bXyQvnV"
   },
   "source": [
    "# Compute performance metrics for the given Y and Y_score without sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "4CHb6NE7Qvnc"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "# other than these two you should not import any other packages"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KbsWXuDaQvnq"
   },
   "source": [
    "\n",
    "## A. Compute performance metrics for the given data '5_a.csv'\n",
    " <pre>  <b>Note 1:</b> in this data you can see number of positive points >> number of negatives points\n",
    "   <b>Note 2:</b> use pandas or numpy to read the data from <b>5_a.csv</b>\n",
    "   <b>Note 3:</b> you need to derive the class labels from given score</pre> $y^{pred}= \\text{[0 if y_score < 0.5 else 1]}$\n",
    "\n",
    "<pre>\n",
    "<ol>\n",
    "<li> Compute Confusion Matrix </li>\n",
    "<li> Compute F1 Score </li>\n",
    "<li> Compute AUC Score, you need to compute different thresholds and for each threshold compute tpr,fpr and then use               numpy.trapz(tpr_array, fpr_array) <a href='https://stackoverflow.com/q/53603376/4084039'>https://stackoverflow.com/q/53603376/4084039</a>, <a href='https://stackoverflow.com/a/39678975/4084039'>https://stackoverflow.com/a/39678975/4084039</a> Note: it should be numpy.trapz(tpr_array, fpr_array) not numpy.trapz(fpr_array, tpr_array)\n",
    "Note- Make sure that you arrange your probability scores in descending order while calculating AUC</li>\n",
    "<li> Compute Accuracy Score </li>\n",
    "</ol>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "WaFLW7oBQvnt"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>proba</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.637387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.635165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.766586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.724564</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.889199</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     y     proba\n",
       "0  1.0  0.637387\n",
       "1  1.0  0.635165\n",
       "2  1.0  0.766586\n",
       "3  1.0  0.724564\n",
       "4  1.0  0.889199"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_a = pd.read_csv('5_a.csv')\n",
    "df_a.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    10100\n",
       "Name: y_pred, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# https://numpy.org/doc/stable/reference/generated/numpy.where.html#numpy-where\n",
    "\n",
    "df_a['y_pred'] = np.where(df_a['proba'] > .5, 1, 0)\n",
    "df_a['y_pred'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://scikit-learn.org/stable/_images/sphx_glr_plot_confusion_matrix_001.png\n",
    "\n",
    "def confusion_matrix_scores(df, actual, predicted):\n",
    "    true_neg = df[(df[actual] == 0) & (df[predicted] == 0)].shape[0]\n",
    "    false_pos = df[(df[actual] == 0) & (df[predicted] == 1)].shape[0]\n",
    "\n",
    "    true_pos = df[(df[actual] == 1) & (df[predicted] == 1)].shape[0]\n",
    "    false_neg = df[(df[actual] == 1) & (df[predicted] == 0)].shape[0]\n",
    "    \n",
    "    df_confusion_mat = pd.DataFrame({'Predicted No': [true_neg, false_neg],\n",
    "                       'Predicted Yes' :[false_pos, true_pos]},\n",
    "                     index = ['Actual No', 'Actual Yes'])\n",
    "    \n",
    "    return true_neg, false_pos, false_neg, true_pos, df_confusion_mat\n",
    "\n",
    "# true_negative, false_positive, false_negative, true_positive, df_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix : [[0, 100], [0, 10000]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted No</th>\n",
       "      <th>Predicted Yes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Actual No</th>\n",
       "      <td>0</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual Yes</th>\n",
       "      <td>0</td>\n",
       "      <td>10000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Predicted No  Predicted Yes\n",
       "Actual No              0            100\n",
       "Actual Yes             0          10000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Compute Confusion Matrix\n",
    "\n",
    "true_negative, false_positive, false_negative, true_positive, df_confusion_matrix = \\\n",
    "                                                confusion_matrix_scores(df_a, 'y', 'y_pred')\n",
    "\n",
    "# true_negative = df_a[(df_a.y == 0) & (df_a.y_pred == 0)].shape[0]\n",
    "# false_positive = df_a[(df_a.y == 0) & (df_a.y_pred == 1)].shape[0]\n",
    "\n",
    "# true_positive = df_a[(df_a.y == 1) & (df_a.y_pred == 1)].shape[0]\n",
    "# false_negative = df_a[(df_a.y == 1) & (df_a.y_pred == 0)].shape[0]\n",
    "\n",
    "confusion_matrix = [[true_negative, false_positive], [false_negative, true_positive]]\n",
    "print('Confusion Matrix :' ,confusion_matrix)\n",
    "\n",
    "df_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score : 0.99502\n"
     ]
    }
   ],
   "source": [
    "# 2. Compute F1 Score\n",
    "\n",
    "def f1_score_calc(true_pos, false_pos, false_neg):\n",
    "    '''\n",
    "    Computes F1-Score\n",
    "    \n",
    "    Input : true_positive, false_positive, false_negative\n",
    "    \n",
    "    Output : F1-Score\n",
    "    '''\n",
    "    precission = true_pos / (true_pos + false_pos)\n",
    "    recall = true_pos / (true_pos + false_neg)\n",
    "    f1_score = 2 * ((precission * recall) / (precission + recall))\n",
    "    \n",
    "    return round(f1_score, 5)\n",
    "\n",
    "f1_score_ = f1_score_calc(true_positive, false_positive, false_negative)\n",
    "\n",
    "print('F1-Score :', round(f1_score_, 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.9901\n"
     ]
    }
   ],
   "source": [
    "# 4. Compute Accuracy Score\n",
    "\n",
    "def accuracy_score(true_pos, true_neg, df):\n",
    "    '''\n",
    "    Computes Accuracy Score\n",
    "    \n",
    "    Input : true_positive, true_negative, dataframe\n",
    "    \n",
    "    Output : Accuracy Score\n",
    "    '''\n",
    "    accuracy_sco = (true_pos + true_neg) / df.shape[0]\n",
    "    \n",
    "    return round(accuracy_sco, 5)\n",
    "\n",
    "accuracy_ = accuracy_score(true_positive, true_negative, df_a)\n",
    "\n",
    "print('Accuracy :', accuracy_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC Score : 0.4883\n"
     ]
    }
   ],
   "source": [
    "# 3. Compute AUC Score\n",
    "\n",
    "# print(df_a.proba.unique())\n",
    "# print(sorted(df_a.proba.unique(), reverse = False)[:5])\n",
    "\n",
    "def compute_auc_score(df):\n",
    "    '''\n",
    "    Computes AUC Score\n",
    "    \n",
    "    Input : dataframe\n",
    "    \n",
    "    Output : AUC Score\n",
    "    '''\n",
    "    thresholds_list = sorted(df.proba.unique(), reverse = True)\n",
    "\n",
    "    #lists for storing true_positive & false_positive rates\n",
    "    true_positive_rate_list = []\n",
    "    false_positive_rate_list = []\n",
    "\n",
    "#     print(df.head())\n",
    "    for threshold in thresholds_list:\n",
    "\n",
    "        # Mapping predictions based on threshold value\n",
    "        df['threshold'] = np.where(df['proba'] <= threshold, 0, 1)  \n",
    "        \n",
    "        true_negative, false_positive, false_negative, true_positive, df_confusion_matrix = \\\n",
    "                                                    confusion_matrix_scores(df ,'y', 'threshold')\n",
    "\n",
    "        true_positive_rate = true_positive / (true_positive + false_negative)\n",
    "        false_positive_rate = false_positive / (true_negative + false_positive)\n",
    "\n",
    "        true_positive_rate_list.append(true_positive_rate)\n",
    "        false_positive_rate_list.append(false_positive_rate)\n",
    "\n",
    "    '''\n",
    "    df['TP_rate'] = true_positive_rate_list\n",
    "    df['FP_rate'] = false_positive_rate_list\n",
    "    '''\n",
    "    auc_score = np.trapz(true_positive_rate_list,false_positive_rate_list)\n",
    "\n",
    "    return round(auc_score, 5)\n",
    "    \n",
    "auc_score_ = compute_auc_score(df_a)\n",
    "\n",
    "print('AUC Score :', auc_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "V5KZem1BQvn2"
   },
   "source": [
    "\n",
    "\n",
    "## B. Compute performance metrics for the given data '5_b.csv'\n",
    "<pre>\n",
    "   <b>Note 1:</b> in this data you can see number of positive points << number of negatives points\n",
    "   <b>Note 2:</b> use pandas or numpy to read the data from <b>5_b.csv</b>\n",
    "   <b>Note 3:</b> you need to derive the class labels from given score</pre> $y^{pred}= \\text{[0 if y_score < 0.5 else 1]}$\n",
    "\n",
    "<pre>\n",
    "<ol>\n",
    "<li> Compute Confusion Matrix </li>\n",
    "<li> Compute F1 Score </li>\n",
    "<li> Compute AUC Score, you need to compute different thresholds and for each threshold compute tpr,fpr and then use               numpy.trapz(tpr_array, fpr_array) <a href='https://stackoverflow.com/q/53603376/4084039'>https://stackoverflow.com/q/53603376/4084039</a>, <a href='https://stackoverflow.com/a/39678975/4084039'>https://stackoverflow.com/a/39678975/4084039</a>\n",
    "Note- Make sure that you arrange your probability scores in descending order while calculating AUC</li>\n",
    "<li> Compute Accuracy Score </li>\n",
    "</ol>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "U2sKlq0YQvn5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>proba</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.281035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.465152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.352793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.157818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.276648</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     y     proba\n",
       "0  0.0  0.281035\n",
       "1  0.0  0.465152\n",
       "2  0.0  0.352793\n",
       "3  0.0  0.157818\n",
       "4  0.0  0.276648"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_b=pd.read_csv('5_b.csv')\n",
    "df_b.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "xlLVa-cVAfCS"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    9806\n",
       "1     294\n",
       "Name: y_pred, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# write your code here for task B\n",
    "\n",
    "df_b['y_pred'] = np.where(df_b['proba'] > .5, 1, 0)\n",
    "df_b['y_pred'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix : [[9761, 239], [45, 55]]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Predicted No</th>\n",
       "      <th>Predicted Yes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Actual No</th>\n",
       "      <td>9761</td>\n",
       "      <td>239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Actual Yes</th>\n",
       "      <td>45</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Predicted No  Predicted Yes\n",
       "Actual No           9761            239\n",
       "Actual Yes            45             55"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Compute Confusion Matrix\n",
    "\n",
    "# https://scikit-learn.org/stable/_images/sphx_glr_plot_confusion_matrix_001.png\n",
    "\n",
    "true_negative, false_positive, false_negative, true_positive, df_confusion_matrix = \\\n",
    "                                                confusion_matrix_scores(df_b, 'y', 'y_pred')\n",
    "\n",
    "confusion_matrix = [[true_negative, false_positive], [false_negative, true_positive]]\n",
    "print('Confusion Matrix :' ,confusion_matrix)\n",
    "\n",
    "df_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-Score : 0.27919\n"
     ]
    }
   ],
   "source": [
    "# 2. Compute F1 Score\n",
    "\n",
    "f1_score_ = f1_score_calc(true_positive, false_positive, false_negative)\n",
    "\n",
    "print('F1-Score :', f1_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy : 0.97188\n"
     ]
    }
   ],
   "source": [
    "# 4. Compute Accuracy Score\n",
    "\n",
    "accuracy_ = accuracy_score(true_positive, true_negative, df_b)\n",
    "\n",
    "print('Accuracy :', accuracy_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC Score : 0.93766\n"
     ]
    }
   ],
   "source": [
    "# 3. Compute AUC Score\n",
    "\n",
    "auc_score_ = compute_auc_score(df_b)\n",
    "\n",
    "print('AUC Score :', auc_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GiPGonTzQvoB"
   },
   "source": [
    "### C. Compute the best threshold (similarly to ROC curve computation) of probability which gives lowest values of metric <b>A</b> for the given data \n",
    "<br>\n",
    "\n",
    "you will be predicting label of a data points like this: $y^{pred}= \\text{[0 if y_score < threshold  else 1]}$\n",
    "\n",
    "$ A = 500 \\times \\text{number of false negative} + 100 \\times \\text{numebr of false positive}$\n",
    "\n",
    "<pre>\n",
    "   <b>Note 1:</b> in this data you can see number of negative points > number of positive points\n",
    "   <b>Note 2:</b> use pandas or numpy to read the data from <b>5_c.csv</b>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "x5HIJzq1QvoE"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.458521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0.505037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>0.418652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0.412057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>0.375579</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   y      prob\n",
       "0  0  0.458521\n",
       "1  0  0.505037\n",
       "2  0  0.418652\n",
       "3  0  0.412057\n",
       "4  0  0.375579"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_c = pd.read_csv('5_c.csv')\n",
    "df_c.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "eAPjewjzAfCa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum 'A' at location 141000 and threshold value at that point is 0.22987\n"
     ]
    }
   ],
   "source": [
    "# write your code for task C\n",
    "\n",
    "thresholds_list = sorted(df_c.prob.unique(), reverse = True)\n",
    "\n",
    "#lists for storing valuation_metric scores\n",
    "valuation_metric_list = []\n",
    "\n",
    "for threshold in thresholds_list:\n",
    "\n",
    "#     Mapping predictions based on threshold value\n",
    "    df_c['threshold'] = np.where(df_c['prob'] <= threshold, 0, 1)\n",
    "\n",
    "#     confusion_matrix_scores calculation\n",
    "    true_negative, false_positive, false_negative, true_positive, df_confusion_matrix = \\\n",
    "                                                confusion_matrix_scores(df_c ,'y', 'threshold')\n",
    "\n",
    "    valuation_metric = ((500 * false_negative) + (100 * false_positive))\n",
    "    \n",
    "#     storing valuation_metric scores\n",
    "    valuation_metric_list.append(valuation_metric)\n",
    "    \n",
    "# https://numpy.org/doc/stable/reference/generated/numpy.argmin.html\n",
    "minimun_value_position = np.argmin(valuation_metric_list)\n",
    "\n",
    "print(f\"Minimum 'A' at location {valuation_metric_list[minimun_value_position]} and threshold\\\n",
    " value at that point is {round(thresholds_list[minimun_value_position],5)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sD4CcgjXQvoL"
   },
   "source": [
    "\n",
    "## D.</b></font> Compute performance metrics(for regression) for the given data 5_d.csv\n",
    "<pre>    <b>Note 2:</b> use pandas or numpy to read the data from <b>5_d.csv</b>\n",
    "    <b>Note 1:</b> <b>5_d.csv</b> will having two columns Y and predicted_Y both are real valued features\n",
    "<ol>\n",
    "<li> Compute Mean Square Error </li>\n",
    "<li> Compute MAPE: https://www.youtube.com/watch?v=ly6ztgIkUxk</li>\n",
    "<li> Compute R^2 error: https://en.wikipedia.org/wiki/Coefficient_of_determination#Definitions </li>\n",
    "</ol>\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "sVOj-bF9AfCd"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>y</th>\n",
       "      <th>pred</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>101.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>120.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>131.0</td>\n",
       "      <td>113.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>164.0</td>\n",
       "      <td>125.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>154.0</td>\n",
       "      <td>152.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       y   pred\n",
       "0  101.0  100.0\n",
       "1  120.0  100.0\n",
       "2  131.0  113.0\n",
       "3  164.0  125.0\n",
       "4  154.0  152.0"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_d=pd.read_csv('5_d.csv')\n",
    "df_d.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "uRhL1pheAfCe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Square Error :  177.1657\n"
     ]
    }
   ],
   "source": [
    "# write your code for task 5d\n",
    "\n",
    "# 1. Compute Mean Square Error\n",
    "\n",
    "# Splitting data into 2 lists\n",
    "act_y = df_d.y\n",
    "pred_y = df_d.pred\n",
    "# print('Length of act_y\\t\\t\\t: ', len(act_y))\n",
    "# print('Length of pred_y\\t\\t: ', len(pred_y))\n",
    "\n",
    "diff_act_y_pred_y = act_y - pred_y\n",
    "# print('Length of diff_act_y_pred_y\\t: ',len(diff_act_y_pred_y))\n",
    "\n",
    "# https://numpy.org/doc/stable/reference/generated/numpy.power.html\n",
    "\n",
    "mean_square_error = np.mean(np.power(diff_act_y_pred_y, 2))\n",
    "print('Mean Square Error : ', round(mean_square_error,5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAPE error is 12.91203 %\n"
     ]
    }
   ],
   "source": [
    "# 2. Compute MAPE\n",
    "\n",
    "# https://numpy.org/doc/stable/reference/generated/numpy.absolute.html\n",
    "\n",
    "abs_diff_act_y_pred_y = np.absolute(diff_act_y_pred_y)\n",
    "mean_act_y = np.mean(act_y)\n",
    "\n",
    "mape_score = np.mean(abs_diff_act_y_pred_y / mean_act_y)\n",
    "\n",
    "# since it's a percentage error, multiplying by 100\n",
    "print(f'MAPE error is {round(mape_score * 100, 5)} %')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Coefficient of determination OR R^2 value is : 0.95636\n"
     ]
    }
   ],
   "source": [
    "# 3. Compute R^2 error: https://en.wikipedia.org/wiki/Coefficient_of_determination#Definitions \n",
    "\n",
    "residual_sum_square_error = np.power(diff_act_y_pred_y, 2).sum()\n",
    "total_sum_of_squares = np.power((act_y - mean_act_y),2).sum()\n",
    "\n",
    "r2_score = 1 - (residual_sum_square_error/total_sum_of_squares)\n",
    "\n",
    "print('Coefficient of determination OR R^2 value is :', round(r2_score, 5))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "5_Performance_metrics_Instructions.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
