{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "92cYIeQF0bAQ"
   },
   "source": [
    "# Segmentation of Indian Traffic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5gG_10XW0bAR"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "from PIL import Image, ImageDraw\n",
    "from PIL import ImagePath\n",
    "import pandas as pd\n",
    "import os\n",
    "from os import path\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import urllib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bl2voXpM0bAW"
   },
   "source": [
    "<pre>\n",
    "1. You can download the data from this link, and extract it\n",
    "\n",
    "2. All your data will be in the folder \"data\" \n",
    "\n",
    "3. Inside the data you will be having two folders\n",
    "\n",
    "|--- data\n",
    "|-----| ---- images\n",
    "|-----| ------|----- Scene 1\n",
    "|-----| ------|--------| ----- Frame 1 (image 1)\n",
    "|-----| ------|--------| ----- Frame 2 (image 2)\n",
    "|-----| ------|--------| ----- ...\n",
    "|-----| ------|----- Scene 2\n",
    "|-----| ------|--------| ----- Frame 1 (image 1)\n",
    "|-----| ------|--------| ----- Frame 2 (image 2)\n",
    "|-----| ------|--------| ----- ...\n",
    "|-----| ------|----- .....\n",
    "|-----| ---- masks\n",
    "|-----| ------|----- Scene 1\n",
    "|-----| ------|--------| ----- json 1 (labeled objects in image 1)\n",
    "|-----| ------|--------| ----- json 2 (labeled objects in image 1)\n",
    "|-----| ------|--------| ----- ...\n",
    "|-----| ------|----- Scene 2\n",
    "|-----| ------|--------| ----- json 1 (labeled objects in image 1)\n",
    "|-----| ------|--------| ----- json 2 (labeled objects in image 1)\n",
    "|-----| ------|--------| ----- ...\n",
    "|-----| ------|----- .....\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hWCQ4I4d0bAX"
   },
   "source": [
    "# Task 1: Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r4owX5YO0bAY"
   },
   "source": [
    "## 1. Get all the file name and corresponding json files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p-Q8onrE0bAZ"
   },
   "outputs": [],
   "source": [
    "def return_file_names_df(root_dir):\n",
    "    # write the code that will create a dataframe with two columns ['images', 'json']\n",
    "    # the column 'image' will have path to images\n",
    "    # the column 'json' will have path to json files\n",
    "    \n",
    "    return data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Em4n2bW10bAc",
    "outputId": "bbc97e7a-e093-40e3-b5e8-17947816146e",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>json</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>data/images/201/frame0029_leftImg8bit.jpg</td>\n",
       "      <td>data/mask/201/frame0029_gtFine_polygons.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>data/images/201/frame0299_leftImg8bit.jpg</td>\n",
       "      <td>data/mask/201/frame0299_gtFine_polygons.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>data/images/201/frame0779_leftImg8bit.jpg</td>\n",
       "      <td>data/mask/201/frame0779_gtFine_polygons.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>data/images/201/frame1019_leftImg8bit.jpg</td>\n",
       "      <td>data/mask/201/frame1019_gtFine_polygons.json</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>data/images/201/frame1469_leftImg8bit.jpg</td>\n",
       "      <td>data/mask/201/frame1469_gtFine_polygons.json</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       image  \\\n",
       "0  data/images/201/frame0029_leftImg8bit.jpg   \n",
       "1  data/images/201/frame0299_leftImg8bit.jpg   \n",
       "2  data/images/201/frame0779_leftImg8bit.jpg   \n",
       "3  data/images/201/frame1019_leftImg8bit.jpg   \n",
       "4  data/images/201/frame1469_leftImg8bit.jpg   \n",
       "\n",
       "                                           json  \n",
       "0  data/mask/201/frame0029_gtFine_polygons.json  \n",
       "1  data/mask/201/frame0299_gtFine_polygons.json  \n",
       "2  data/mask/201/frame0779_gtFine_polygons.json  \n",
       "3  data/mask/201/frame1019_gtFine_polygons.json  \n",
       "4  data/mask/201/frame1469_gtFine_polygons.json  "
      ]
     },
     "execution_count": 6,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df = return_file_names_df(root_dir)\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uof88SJW0bAf"
   },
   "source": [
    "> If you observe the dataframe, we can consider each row as single data point, where first feature is image and the second feature is corresponding json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OjMprsim0bAg"
   },
   "outputs": [],
   "source": [
    "\n",
    "def grader_1(data_df):\n",
    "    for i in data_df.values:\n",
    "        if not (path.isfile(i[0]) and path.isfile(i[1]) and i[0][12:i[0].find('_')]==i[1][10:i[1].find('_')]):\n",
    "            return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_fvgVQWF0bAj",
    "outputId": "8ed653b4-ee69-466e-ae08-1897fd3a8316"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grader_1(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0ijL77Y10bAl",
    "outputId": "df272242-749a-410e-c29a-0fc761a0db39"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4008, 2)"
      ]
     },
     "execution_count": 13,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DB2-yrRV0bAo"
   },
   "source": [
    "## 2. Structure of sample Json file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DcqXYpEI0bAp"
   },
   "source": [
    "<img src='https://i.imgur.com/EfR5KmI.png' width=\"200\" height=\"100\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "97ZcwNbr0bAp"
   },
   "source": [
    "* Each File will have 3 attributes\n",
    "    * imgHeight: which tells the height of the image\n",
    "    * imgWidth: which tells the width of the image\n",
    "    * objects: it is a list of objects, each object will have multiple attributes,\n",
    "        * label: the type of the object\n",
    "        * polygon: a list of two element lists, representing the coordinates of the polygon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pYrICucM0bAq"
   },
   "source": [
    "#### Compute the unique labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t-OYvoA20bAr"
   },
   "source": [
    "Let's see how many unique objects are there in the json file.\n",
    "to see how to get the object from the json file please check <a href='https://www.geeksforgeeks.org/read-json-file-using-python/'>this blog </a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5su9envb0bAr"
   },
   "outputs": [],
   "source": [
    "def return_unique_labels(data_df):\n",
    "    # for each file in the column json\n",
    "    #       read and store all the objects present in that file\n",
    "    # compute the unique objects and retrun them\n",
    "    # if open any json file using any editor you will get better sense of it\n",
    "    return unique_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ch8hYKOS0bAu"
   },
   "outputs": [],
   "source": [
    "unique_labels = return_unique_labels(data_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6mSHuX4Y0bAw"
   },
   "source": [
    "<img src='https://i.imgur.com/L4QH6Tp.png'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gnbt7MSc0bAw"
   },
   "outputs": [],
   "source": [
    "label_clr = {'road':10, 'parking':20, 'drivable fallback':20,'sidewalk':30,'non-drivable fallback':40,'rail track':40,\\\n",
    "                        'person':50, 'animal':50, 'rider':60, 'motorcycle':70, 'bicycle':70, 'autorickshaw':80,\\\n",
    "                        'car':80, 'truck':90, 'bus':90, 'vehicle fallback':90, 'trailer':90, 'caravan':90,\\\n",
    "                        'curb':100, 'wall':100, 'fence':110,'guard rail':110, 'billboard':120,'traffic sign':120,\\\n",
    "                        'traffic light':120, 'pole':130, 'polegroup':130, 'obs-str-bar-fallback':130,'building':140,\\\n",
    "                        'bridge':140,'tunnel':140, 'vegetation':150, 'sky':160, 'fallback background':160,'unlabeled':0,\\\n",
    "                        'out of roi':0, 'ego vehicle':170, 'ground':180,'rectification border':190,\\\n",
    "                   'train':200}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qPuuN7UB0bAz",
    "outputId": "d7160c83-f4e0-4515-f315-bf2fad0bf84f",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "def grader_2(unique_labels):\n",
    "    if (not (set(label_clr.keys())-set(unique_labels))) and len(unique_labels) == 40:\n",
    "        print(\"True\")\n",
    "    else:\n",
    "        print(\"Flase\")\n",
    "\n",
    "grader_2(unique_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3xmUIzX00bA2"
   },
   "source": [
    "<pre>\n",
    "* here we have given a number for each of object types, if you see we are having 21 different set of objects\n",
    "* Note that we have multiplies each object's number with 10, that is just to make different objects look differently in the segmentation map\n",
    "* Before you pass it to the models, you might need to devide the image array /10.\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RzUK6FMJ0bA2"
   },
   "source": [
    "## 3. Extracting the polygons from the json files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oRRcwMw50bA3"
   },
   "outputs": [],
   "source": [
    "def get_poly(file):\n",
    "    # this function will take a file name as argument\n",
    "    \n",
    "    # it will process all the objects in that file and returns\n",
    "    \n",
    "    # label: a list of labels for all the objects label[i] will have the corresponding vertices in vertexlist[i]\n",
    "    # len(label) == number of objects in the image\n",
    "    \n",
    "    # vertexlist: it should be list of list of vertices in tuple formate \n",
    "    # ex: [[(x11,y11), (x12,y12), (x13,y13) .. (x1n,y1n)]\n",
    "    #     [(x21,y21), (x22,y12), (x23,y23) .. (x2n,y2n)]\n",
    "    #      .....\n",
    "    #     [(xm1,ym1), (xm2,ym2), (xm3,ym3) .. (xmn,ymn)]]\n",
    "    # len(vertexlist) == number of objects in the image\n",
    "    \n",
    "    # * note that label[i] and vertextlist[i] are corresponds to the same object, one represents the type of the object\n",
    "    # the other represents the location\n",
    "    \n",
    "    # width of the image\n",
    "    # height of the image\n",
    "    return w, h, label, vertexlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FlcCswQR0bA5",
    "outputId": "8d522c26-b4e2-4fcb-f71a-2e2d493e60df",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "def grader_3(file):\n",
    "    w, h, labels, vertexlist = get_poly(file)\n",
    "    print(len((set(labels)))==18 and len(vertexlist)==227 and w==1920 and h==1080 \\\n",
    "          and isinstance(vertexlist,list) and isinstance(vertexlist[0],list) and isinstance(vertexlist[0][0],tuple) )\n",
    "\n",
    "grader_3('data/mask/201/frame0029_gtFine_polygons.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lw2MH_ua0bA8"
   },
   "source": [
    "## 4. Creating Image segmentations by drawing set of polygons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mLrtu1f80bA8"
   },
   "source": [
    "### Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-O7l9Xfj0bA9",
    "outputId": "54940e05-d61c-4168-a9d7-fb13d786e27c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(28, 28, 3)\n",
      "[[0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 0 0 0 0 0 0 0 0 0]\n",
      " [2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 0 0 0 0 0 0 0 0 0]\n",
      " [2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 0 0 0 0 0 0 0 0]\n",
      " [0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 2 2 2 2 2 2 2 2 2 2 2 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 2 2 2 2 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 3 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 3 3 3 3 3 3 3 3 3 3 3 3 3 3 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 0]\n",
      " [0 0 0 0 0 0 0 0 0 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 0]\n",
      " [0 0 0 0 0 0 0 0 0 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3]\n",
      " [0 0 0 0 0 0 0 0 0 0 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 3 3 3 3 3 3 3 3 3 3 3 3 3 3 3 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 3 3 3 3 3 3 3 3 3 3 3 3 3 3 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 3 3 3 3 3 3 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import math \n",
    "from PIL import Image, ImageDraw \n",
    "from PIL import ImagePath  \n",
    "side=8\n",
    "x1 = [ ((math.cos(th) + 1) *9, (math.sin(th) + 1) * 6) for th in [i * (2 * math.pi) / side for i in range(side)] ]\n",
    "x2 = [ ((math.cos(th) + 2) *9, (math.sin(th) + 3) *6) for th in [i * (2 * math.pi) / side for i in range(side)] ]\n",
    "\n",
    "img = Image.new(\"RGB\", (28,28))\n",
    "img1 = ImageDraw.Draw(img)\n",
    "# please play with the fill value\n",
    "# writing the first polygon\n",
    "img1.polygon(x1, fill =20)\n",
    "# writing the second polygon\n",
    "img1.polygon(x2, fill =30)\n",
    "\n",
    "img=np.array(img)\n",
    "# note that the filling of the values happens at the channel 1, so we are considering only the first channel here\n",
    "plt.imshow(img[:,:,0])\n",
    "print(img.shape)\n",
    "print(img[:,:,0]//10)\n",
    "im = Image.fromarray(img[:,:,0])\n",
    "im.save(\"test_image.png\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bnAzJJhq0bBB"
   },
   "outputs": [],
   "source": [
    "def compute_masks(data_df):\n",
    "    # after you have computed the vertexlist plot that polygone in image like this\n",
    "    \n",
    "    # img = Image.new(\"RGB\", (w, h))\n",
    "    # img1 = ImageDraw.Draw(img)\n",
    "    # img1.polygon(vertexlist[i], fill = label_clr[label[i]])\n",
    "    \n",
    "    # after drawing all the polygons that we collected from json file, \n",
    "    # you need to store that image in the folder like this \"data/output/scene/framenumber_gtFine_polygons.png\"\n",
    "    \n",
    "    # after saving the image into disk, store the path in a list\n",
    "    # after storing all the paths, add a column to the data_df['mask'] ex: data_df['mask']= mask_paths\n",
    "    \n",
    "    return data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hkDgQM280bBI",
    "outputId": "7058d5b5-f779-436f-dbf7-605c5a814707"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 4008/4008 [04:16<00:00, 15.61it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>json</th>\n",
       "      <th>mask</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>data/images/201/frame0029_leftImg8bit.jpg</td>\n",
       "      <td>data/mask/201/frame0029_gtFine_polygons.json</td>\n",
       "      <td>data/output/201/frame0029_gtFine_polygons.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>data/images/201/frame0299_leftImg8bit.jpg</td>\n",
       "      <td>data/mask/201/frame0299_gtFine_polygons.json</td>\n",
       "      <td>data/output/201/frame0299_gtFine_polygons.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>data/images/201/frame0779_leftImg8bit.jpg</td>\n",
       "      <td>data/mask/201/frame0779_gtFine_polygons.json</td>\n",
       "      <td>data/output/201/frame0779_gtFine_polygons.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>data/images/201/frame1019_leftImg8bit.jpg</td>\n",
       "      <td>data/mask/201/frame1019_gtFine_polygons.json</td>\n",
       "      <td>data/output/201/frame1019_gtFine_polygons.png</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>data/images/201/frame1469_leftImg8bit.jpg</td>\n",
       "      <td>data/mask/201/frame1469_gtFine_polygons.json</td>\n",
       "      <td>data/output/201/frame1469_gtFine_polygons.png</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       image  \\\n",
       "0  data/images/201/frame0029_leftImg8bit.jpg   \n",
       "1  data/images/201/frame0299_leftImg8bit.jpg   \n",
       "2  data/images/201/frame0779_leftImg8bit.jpg   \n",
       "3  data/images/201/frame1019_leftImg8bit.jpg   \n",
       "4  data/images/201/frame1469_leftImg8bit.jpg   \n",
       "\n",
       "                                           json  \\\n",
       "0  data/mask/201/frame0029_gtFine_polygons.json   \n",
       "1  data/mask/201/frame0299_gtFine_polygons.json   \n",
       "2  data/mask/201/frame0779_gtFine_polygons.json   \n",
       "3  data/mask/201/frame1019_gtFine_polygons.json   \n",
       "4  data/mask/201/frame1469_gtFine_polygons.json   \n",
       "\n",
       "                                            mask  \n",
       "0  data/output/201/frame0029_gtFine_polygons.png  \n",
       "1  data/output/201/frame0299_gtFine_polygons.png  \n",
       "2  data/output/201/frame0779_gtFine_polygons.png  \n",
       "3  data/output/201/frame1019_gtFine_polygons.png  \n",
       "4  data/output/201/frame1469_gtFine_polygons.png  "
      ]
     },
     "execution_count": 27,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df = compute_masks(data_df)\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yXF31hnZRovh"
   },
   "outputs": [],
   "source": [
    "#daving the final dataframe to a csv file\n",
    "data_df.to_csv('preprocessed_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t9RPr0aZ0bBU"
   },
   "source": [
    "# Task 2: Applying Unet to segment the images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HDRjb4AD0bBV"
   },
   "source": [
    "<pre>\n",
    "* please check the paper: https://arxiv.org/abs/1505.04597\n",
    "\n",
    "* <img src='https://i.imgur.com/rD4yP7J.jpg' width=\"500\">\n",
    "\n",
    "* As a part of this assignment we won't writingt this whole architecture, rather we will be doing transfer learning\n",
    "\n",
    "* please check the library <a hreaf='https://github.com/qubvel/segmentation_models'>https://github.com/qubvel/segmentation_models</a>\n",
    "\n",
    "* You can install it like this \"pip install -U segmentation-models==0.2.1\", even in google colab you can install the    same with \"!pip install -U segmentation-models==0.2.1\" \n",
    "\n",
    "* Check the reference notebook in which we have solved one end to end case study of image forgery detection using same  unet\n",
    "\n",
    "* The number of channels in the output will depend on the number of classes in your data, since we know that we are having 21 classes, the number of channels in the output will also be 21\n",
    "\n",
    "* <strong>This is where we want you to explore, how do you featurize your created segmentation map note that the original map will be of (w, h, 1) and the output will be (w, h, 21) how will you calculate the loss</strong>, you can check the examples in segmentation github\n",
    "\n",
    "* please use the loss function that is used in the refence notebooks\n",
    "\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mrVV7zW60bBW"
   },
   "source": [
    "### Task 2.1: Dice loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1zzYwXRi0bBW"
   },
   "source": [
    "<pre>\n",
    "* Explain the Dice loss\n",
    "* 1. Write the formualtion\n",
    "* 2. Range of the loss function\n",
    "* 3. Interpretation of loss function\n",
    "* 4. Write your understanding of the loss function, how does it helps in segmentation\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "akXZdZBx0bBX"
   },
   "source": [
    "### Task 2.2: Training Unet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U5bKBqDn0bBX"
   },
   "source": [
    "\n",
    "<pre>\n",
    "* Split the data into 80:20.\n",
    "* Train the UNET on the given dataset and plot the train and validation loss.\n",
    "* As shown in the reference notebook plot 20 images from the test data along with its segmentation map, predicted map.\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zTtIHDSf0bBY"
   },
   "source": [
    "# Task 3: Training CANet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lM6PTaqo0bBY",
    "outputId": "d6d44dc2-2471-4368-ecad-204a26be17ae"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\installed\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "D:\\installed\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "D:\\installed\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "D:\\installed\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "D:\\installed\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "D:\\installed\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "# tf.compat.v1.enable_eager_execution()\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.layers import UpSampling2D\n",
    "from tensorflow.keras.layers import MaxPooling2D, GlobalAveragePooling2D\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from tensorflow.keras.layers import Multiply\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import Input, Add, Dense, Activation, ZeroPadding2D, BatchNormalization, Flatten, Conv2D, AveragePooling2D, MaxPooling2D, GlobalMaxPooling2D\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.initializers import glorot_uniform\n",
    "K.set_image_data_format('channels_last')\n",
    "K.set_learning_phase(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f0DIdIxO0bBb"
   },
   "source": [
    "* as a part of this assignment we will be implementing the architecture based on this paper https://arxiv.org/pdf/2002.12041.pdf\n",
    "* We will be using the custom layers concept that we used in seq-seq assignment\n",
    "* You can devide the whole architecture can be devided into two parts\n",
    "    1. Encoder\n",
    "    2. Decoder\n",
    "    <img src='https://i.imgur.com/prH3Mno.png' width=\"600\">\n",
    "* Encoder:\n",
    "    * The first step of the encoder is to create the channel maps [$C_1$, $C_2$, $C_3$, $C_4$]\n",
    "    * $C_1$ width and heigths are 4x times less than the original image\n",
    "    * $C_2$ width and heigths are 8x times less than the original image\n",
    "    * $C_3$ width and heigths are 8x times less than the original image\n",
    "    * $C_4$ width and heigths are 8x times less than the original image\n",
    "    * <i>you can reduce the dimensions by using stride parameter</i>.\n",
    "    * [$C_1$, $C_2$, $C_3$, $C_4$] are formed by applying a \"conv block\" followed by $k$ number of \"identity block\". i.e the $C_k$ feature map will single \"conv block\" followed by $k$ number of \"identity blocks\".\n",
    "    <table>\n",
    "    <tr><td><img src=\"https://i.imgur.com/R8Gdypo.png\" width=\"300\"></td>\n",
    "        <td><img src=\"https://i.imgur.com/KNunjQK.png\" width=\"250\"></td></tr>\n",
    "    </table>\n",
    "    * <strong>The conv block and identity block of $C_1$</strong>: the number filters in the covolutional layers will be $[4,4,8]$ and the number of filters in the parallel conv layer will also be $8$.\n",
    "    * <strong>The conv block and identity block of $C_2$</strong>: the number filters in the covolutional layers will be $[8,8,16]$ and the number of filters in the parallel conv layer will also be $16$.\n",
    "    * <strong>The conv block and identity block of $C_3$</strong>: the number filters in the covolutional layers will be $[16,16,32]$ and the number of filters in the parallel conv layer will also be $32$.\n",
    "    * <strong>The conv block and identity block of $C_4$</strong>: the number filters in the covolutional layers will be $[32,32,64]$ and the number of filters in the parallel conv layer will also be $64$.\n",
    "    * Here $\\oplus$ represents the elementwise sum\n",
    "    <br>\n",
    "    \n",
    "    <font color=\"red\">NOTE: these filters are of your choice, you can explore more options also</font>\n",
    "    \n",
    "    * Example: if your image is of size $(512, 512, 3)$\n",
    "        * the output after $C_1$ will be $128*128*8$\n",
    "        * the output after $C_2$ will be $64*64*16$\n",
    "        * the output after $C_3$ will be $64*64*32$\n",
    "        * the output after $C_4$ will be $64*64*64$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AMdxrF6p0bBb"
   },
   "outputs": [],
   "source": [
    "class convolutional_block(tf.keras.layers.Layer):\n",
    "    def __init__(self, kernel=3,  filters=[4,4,8], stride=1, name=\"conv block\"):\n",
    "        super().__init__(name=name)\n",
    "        self.F1, self.F2, self.F3 = filters\n",
    "        self.kernel = kernel\n",
    "        self.stride = stride\n",
    "    def call(self, X):\n",
    "        # write the architecutre that was mentioned above\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WZjGHnBf0bBd"
   },
   "outputs": [],
   "source": [
    "class identity_block(tf.keras.layers.Layer):\n",
    "    def __init__(self, kernel=3,  filters=[4,4,8], name=\"identity block\"):\n",
    "        super().__init__(name=name)\n",
    "        self.F1, self.F2, self.F3 = filters\n",
    "        self.kernel = kernel\n",
    "    def call(self, X):\n",
    "        # write the architecutre that was mentioned above\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QNP8W_o90bBg"
   },
   "source": [
    "* The output of the $C_4$ will be passed to $\\text{Chained Context Aggregation Module (CAM)}$\n",
    "<img src='https://i.imgur.com/Bu63AAA.png' width=\"400\">\n",
    "* The CAM module will have two operations names Context flow and Global flow\n",
    "* <strong>The Global flow</strong>: \n",
    "    * as shown in the above figure first we willl apply  <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/layers/GlobalAveragePooling2D\">global avg pooling</a> which results in (#, 1, 1, number_of_filters) then applying <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/layers/BatchNormalization?version=nightly\">BN</a>, <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/layers/ReLU\">RELU</a>, $1*1 \\text{ Conv}$ layer sequentially which results a matrix (#, 1, 1, number_of_filters). Finally apply <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/layers/UpSampling2D\">upsampling</a> / <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2DTranspose\">conv2d transpose</a> to make the output same as the input dimensions (#, input_height, input_width, number_of_filters)\n",
    "    * If you use <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/layers/UpSampling2D\">upsampling</a> then use bilinear pooling as interpolation technique\n",
    "* <strong>The Context flow</strong>: \n",
    "    * as shown in the above figure (c) the context flow will get inputs from two modules `a. C4` `b. From the above flow` \n",
    "    * We will be <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/layers/Concatenate\">concatinating</a> the both inputs on the last axis.\n",
    "    * After the concatination we will be applying <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/layers/AveragePooling2D\"> Average pooling </a> which reduces the size of feature map by $N\\times$ times\n",
    "    * In the paper it was mentioned that to apply a group convolutions, but for the assignment we will be applying the simple conv layers with kernel size $(3*3)$\n",
    "    * We are skipping the channel shuffling \n",
    "    * similarly we will be applying a simple conv layers with kernel size $(3*3)$ consider this output is X\n",
    "    * later we will get the Y=(X $\\otimes \\sigma((1\\times1)conv(relu((1\\times1)conv(X))))) \\oplus X$, here $\\oplus$ is elementwise addition and $\\otimes$ is elementwise multiplication\n",
    "    * Finally apply <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/layers/UpSampling2D\">upsampling</a> / <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/layers/Conv2DTranspose\">conv2d transpose</a> to make the output same as the input dimensions (#, input_height, input_width, number_of_filters)\n",
    "    * If you use <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/layers/UpSampling2D\">upsampling</a> then use bilinear pooling as interpolation technique\n",
    "\n",
    "NOTE: here N times reduction and N time increments makes the input and out shape same, you can explore with the N values, you can choose N = 2 or 4\n",
    "\n",
    "* Example with N=2:\n",
    "    * Assume the C4 is of shape (64,64,64) then the shape of GF will be (64,64,32)\n",
    "    * Assume the C4 is of shape (64,64,64) and the shape of GF is (64,64,32) then the shape of CF1 will be (64,64,32)\n",
    "    * Assume the C4 is of shape (64,64,64) and the shape of CF1 is (64,64,32) then the shape of CF2 will be (64,64,32)\n",
    "    * Assume the C4 is of shape (64,64,64) and the shape of CF2 is (64,64,32) then the shape of CF3 will be (64,64,32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "p2tIIP730bBg"
   },
   "outputs": [],
   "source": [
    "class global_flow(tf.keras.layers.Layer):\n",
    "    def __init__(self, name=\"global_flow\"):\n",
    "        super().__init__(name=name)\n",
    "        \n",
    "    def call(self, X):\n",
    "        # implement the global flow operatiom\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Oe938KHw0bBk"
   },
   "outputs": [],
   "source": [
    "class context_flow(tf.keras.layers.Layer):    \n",
    "    def __init__(self, name=\"context_flow\"):\n",
    "        super().__init__(name=name)\n",
    "    def call(self, X):\n",
    "        # here X will a list of two elements \n",
    "        INP, FLOW = X[0], X[1] \n",
    "        # implement the context flow as mentioned in the above cell\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kZyxvYZp0bBm"
   },
   "source": [
    "* As shown in the above architecture we will be having 4 context flows\n",
    "* if you have implemented correctly all the shapes of Global Flow, and 3 context flows will have the same dimension\n",
    "* the output of these 4 modules will be <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/layers/Add\">added</a> to get the same output matrix\n",
    "<img src='https://i.imgur.com/Bu63AAA.png' width=\"400\">\n",
    " * The output of after the sum, will be sent to the <strong>Feature selection module $FSM$</strong>\n",
    " \n",
    "* Example:\n",
    "    * if the shapes of GF, CF1, CF2, CF3 are (64,64,32), (64,64,32), (64,64,32), (64,64,32), (64,64,32) respectivly then after the sum we will be getting (64,64,32), which will be passed to the next module.\n",
    " \n",
    "<strong>Feature selection module</strong>:\n",
    "\n",
    "* As part of the FSM we will be applying a conv layer (3,3) with the padding=\"same\" so that the output and input will have same shapes\n",
    "* Let call the output as X\n",
    "* Pass the X to global pooling which results the matrix (#, 1, 1, number_of_channels)\n",
    "* Apply $1*1$ conv layer, after the pooling\n",
    "* the output of the $1*1$ conv layer will be passed to the Batch normalization layer, followed by Sigmoid activation function.\n",
    "* we will be having the output matrix of shape (#, 1, 1, number_of_channels) lets call it 'Y'\n",
    "* <strong>we can interpret this as attention mechanisum, i.e for each channel we will having a weight</strong>\n",
    "* the dimension of X (#, w, h, k) and output above steps Y is (#, 1, 1, k) i.e we need to multiply each channel of X will be <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/layers/Multiply\">multiplied</a> with corresponding channel of Y\n",
    "* After creating the weighted channel map we will be doing upsampling such that it will double the height and width.\n",
    "* apply <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/layers/UpSampling2D\">upsampling</a> with bilinear pooling as interpolation technique\n",
    "\n",
    "* <font color=\"red\">Example</font>:\n",
    "    * Assume the matrix shape of the input is (64,64,32) then after upsampling it will be (128,128,32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dNB8srLR0bBn"
   },
   "outputs": [],
   "source": [
    "class fsm(tf.keras.layers.Layer):    \n",
    "    def __init__(self, name=\"feature_selection\"):\n",
    "        super().__init__(name=name)\n",
    "        \n",
    "    def call(self, X):\n",
    "        # implement the FSM modules based on image in the above cells\n",
    "        return FSM_Conv_T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_Xoqx8w50bBp"
   },
   "source": [
    "* <b>Adapted Global Convolutional Network (AGCN)</b>:\n",
    "    <img src=\"https://i.imgur.com/QNB8RmV.png\" width=\"300\">\n",
    "    \n",
    "    * AGCN will get the input from the output of the \"conv block\" of $C_1$\n",
    "    \n",
    "    * In all the above layers we will be using the padding=\"same\" and stride=(1,1)\n",
    "    \n",
    "    * so that we can have the input and output matrices of same size\n",
    "    \n",
    "* <font color=\"red\">Example</font>:\n",
    "    * Assume the matrix shape of the input is (128,128,32) then the output it will be (128,128,32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dL_T1Muv0bBq"
   },
   "outputs": [],
   "source": [
    "class agcn(tf.keras.layers.Layer):    \n",
    "    def __init__(self, name=\"global_conv_net\"):\n",
    "        super().__init__(name=name)\n",
    "        \n",
    "    def call(self, X):\n",
    "        # please implement the above mentioned architecture\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RStu3wwZ0bBs"
   },
   "source": [
    "*     <img src='https://i.imgur.com/prH3Mno.png' width=\"600\">\n",
    "* as shown in the architecture, after we get the AGCN it will get concatinated with the FSM output\n",
    "\n",
    "* If we observe the shapes both AGCN and FSM will have same height and weight\n",
    "\n",
    "* we will be concatinating both these outputs over the last axis\n",
    "\n",
    "* The concatinated output will be passed to a conv layers with filters = number of classes in our data set and the activation function = 'relu'\n",
    "\n",
    "* we will be using padding=\"same\" which results in the same size feature map\n",
    "\n",
    "* If you observe the shape of matrix, it will be 4x times less than the original image\n",
    "\n",
    "* to make it equal to the original output shape, we will do 4x times upsampling of rows and columns\n",
    "\n",
    "* apply <a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/layers/UpSampling2D\">upsampling</a> with bilinear pooling as interpolation technique\n",
    "\n",
    "* Finally we will be applying sigmoid activation.\n",
    "\n",
    "* Example:\n",
    "    * Assume the matrix shape of AGCN is (128,128,32)  and FSM is (128,128,32) the concatination will make it (128, 128, 64)\n",
    "    * Applying conv layer will make it (128,128,21)\n",
    "    * Finally applying upsampling will make it (512, 512, 21)\n",
    "    * Applying sigmoid will result in the same matrix (512, 512, 21)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O16er1Nx0bBs"
   },
   "outputs": [],
   "source": [
    "X_input = Input(shape=(128,128,3))\n",
    "\n",
    "# Stage 1\n",
    "X = Conv2D(64, (3, 3), name='conv1', padding=\"same\", kernel_initializer=glorot_uniform(seed=0))(X_input)\n",
    "X = BatchNormalization(axis=3, name='bn_conv1')(X)\n",
    "X = Activation('relu')(X)\n",
    "X = MaxPooling2D((2, 2), strides=(2, 2))(X)\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cTOD_qPA0bBv"
   },
   "source": [
    "* If you observe the arcitecture we are creating a feature map with 2x time less width and height\n",
    "* we have written the first stage of the code above.\n",
    "* Write the next layers by using the custom layers we have written"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5UwfJ2tw0bBv",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# write the complete architecutre\n",
    "\n",
    "model = Model(inputs = X, outputs = output)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O9Cmyohz0bBz"
   },
   "outputs": [],
   "source": [
    "tf.keras.utils.plot_model(\n",
    "    model, to_file='model4.png', show_shapes=True, show_layer_names=True,\n",
    "    rankdir='TB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RXnPVQHw0bB1"
   },
   "source": [
    "### Usefull tips:\n",
    "* use \"interpolation=cv2.INTER_NEAREST\" when you are resizing the image, so that it won't mess with the number of classes\n",
    "* keep the images in the square shape like $256*256$ or $512*512$\n",
    "* Carefull when you are converting the (W, H) output image into (W, H, Classes)\n",
    "* Even for the canet, use the segmentation model's losses and the metrics\n",
    "* The goal of this assignment is make you familier in with computer vision problems, image preprocessing, building complex architectures and implementing research papers, so that in future you will be very confident in industry\n",
    "* you can use the tensorboard logss to see how is yours model's training happening\n",
    "* use callbacks that you have implemented in previous assignments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hHWYBsLz3pRY"
   },
   "source": [
    "### Things to keep in mind"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fr7X4N0v1KlO"
   },
   "source": [
    "* You need to train  above built model and plot the train and test losses.\n",
    "* Make sure there is no overfitting, you are free play with the identity blocks in C1, C2, C3, C4\n",
    "* before we apply the final sigmoid activation, you can add more conv layers or BN or dropouts etc\n",
    "* you are free to use any other optimizer or learning rate or weights init or regularizations"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Segmentation_Assignment.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
